{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df)\n",
    "    #print(scaler.mean_)\n",
    "    return scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_values(df):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df)\n",
    "    print(len(le.classes_))\n",
    "    return le.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    for col in ['Geography']:\n",
    "        dataset[col] = encode_values(dataset[col].values)\n",
    "    dataset = dataset.drop(['CustomerId'],axis=1)\n",
    "    dataset = dataset.drop(['Surname'],axis=1)\n",
    "    dataset = dataset.drop(['Gender'],axis=1)\n",
    "    temp_dataset = scale(dataset.drop('Exited',axis=1).values)\n",
    "    new_dataset = pd.DataFrame(temp_dataset,columns=['CreditScore', 'Geography', 'Age', 'Tenure', 'Balance',\n",
    "           'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'])\n",
    "    new_dataset['Exited'] = dataset['Exited']\n",
    "    sample = new_dataset.sample(frac=1)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "np.seterr(over='ignore')\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)  # Seed the random number generator\n",
    "        self.weights = {}  # Create dict to hold weights\n",
    "        self.num_layers = 1  # Set initial number of layer to one (input layer)\n",
    "        self.adjustments = {}  # Create dict to hold adjustements\n",
    "\n",
    "    def add_layer(self, shape):\n",
    "        # Create weights with shape specified + biases\n",
    "        self.weights[self.num_layers] = np.vstack((2 * np.random.random(shape) - 1, 2 * np.random.random((1, shape[1])) - 1))\n",
    "        # Initialize the adjustements for these weights to zero\n",
    "        self.adjustments[self.num_layers] = np.zeros(shape)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def __sigmoid(self, x):\n",
    "        x = np.clip(x,-20,20)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, data):\n",
    "        # Pass data through pretrained network\n",
    "        for layer in range(2, self.num_layers+1):\n",
    "            #print(self.weights[layer-1][:, :-1])\n",
    "            data = np.dot(data.T, self.weights[layer-1][:-1, :]) + self.weights[layer-1][ -1,:].T # + self.biases[layer]\n",
    "            data = self.__sigmoid(data).T\n",
    "        return data\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # Progapagate through network and hold values for use in back-propagation\n",
    "        activation_values = {}\n",
    "        activation_values[1] = data\n",
    "        for layer in range(2, self.num_layers+1):\n",
    "            data = np.dot(data.T, self.weights[layer-1][:-1, :]) + self.weights[layer-1][-1, :].T # + self.biases[layer]\n",
    "            data = self.__sigmoid(data).T\n",
    "            activation_values[layer] = data\n",
    "        return activation_values\n",
    "\n",
    "    def simple_error(self, outputs, targets):\n",
    "        return outputs-targets\n",
    "\n",
    "    def sum_squared_error(self, outputs, targets):\n",
    "        return 0.5 * np.mean(np.sum(np.power(outputs - targets, 2), axis=1))\n",
    "    \n",
    "    def entropy(self,outputs,targets):\n",
    "        return -(1.0/len(targets)) * np.sum(targets*np.log(outputs) + (1-targets)*np.log(1-outputs))\n",
    "\n",
    "    def __back_propagate(self, output, target):\n",
    "        deltas = {}\n",
    "        # Delta of output Layer\n",
    "        deltas[self.num_layers] = self.simple_error(output[self.num_layers],target)\n",
    "\n",
    "        # Delta of hidden Layers\n",
    "        for layer in reversed(range(2, self.num_layers)):  # All layers except input/output\n",
    "            a_val = output[layer]\n",
    "            weights = self.weights[layer][:-1, :]\n",
    "            prev_deltas = deltas[layer+1]\n",
    "            deltas[layer] = np.multiply(np.dot(weights, prev_deltas), self.__sigmoid_derivative(a_val))\n",
    "\n",
    "        # Caclculate total adjustements based on deltas\n",
    "        for layer in range(1, self.num_layers):\n",
    "            self.adjustments[layer] += np.dot(deltas[layer+1], output[layer].T).T\n",
    "\n",
    "    def __gradient_descente(self, batch_size, learning_rate):\n",
    "        # Calculate partial derivative and take a step in that direction\n",
    "        \n",
    "        for layer in range(1, self.num_layers):\n",
    "            partial_d = (1/batch_size) * self.adjustments[layer]\n",
    "            #print(self.weights[layer])\n",
    "            self.weights[layer][:-1, :] += learning_rate * -partial_d\n",
    "            self.weights[layer][-1, :] += learning_rate *1e-3* -partial_d[-1, :]\n",
    "            #print(self.weights[layer])\n",
    "\n",
    "    def train(self, inputs, targets, num_epochs, learning_rate=0.01, stop_accuracy=1e-3):\n",
    "        error = []\n",
    "        for iteration in range(num_epochs):\n",
    "            for i in range(len(inputs)):\n",
    "                x = inputs[i]\n",
    "                y = targets[i]\n",
    "                # Pass the training set through our neural network\n",
    "                output = self.forward_propagate(x)\n",
    "\n",
    "                # Calculate the error\n",
    "                loss = self.sum_squared_error(output[self.num_layers], y)\n",
    "                error.append(loss)\n",
    "\n",
    "                # Calculate Adjustements\n",
    "                self.__back_propagate(output, y)\n",
    "\n",
    "            self.__gradient_descente(i, learning_rate)\n",
    "\n",
    "            # Check if accuarcy criterion is satisfied\n",
    "            if iteration > 0 and iteration %1000 == 0:\n",
    "                print( iteration,np.mean(np.abs(error[-(i+1):])))\n",
    "            if np.mean(np.abs(error[-(i+1):])) < stop_accuracy and iteration > 0:\n",
    "                print( iteration,np.mean(np.abs(error[-(i+1):])))\n",
    "\n",
    "        return(np.asarray(error), iteration+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracy(data,labels):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(data)):\n",
    "        res = nn.predict(data[i])[0]\n",
    "        if res[0] >= 0.5 and labels[i][0] == 1:\n",
    "            TP += 1\n",
    "        elif res[0] < 0.5 and labels[i][0] == 0:\n",
    "            TN += 1\n",
    "        elif res[0] < 0.5 and labels[i][0] == 1:\n",
    "            FN = FN + 1\n",
    "        elif res[0] > 0.5 and labels[i][0] == 0:\n",
    "            FP = FP + 1\n",
    "    Acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "    if (TP+FP == 0):\n",
    "        Prec = 0\n",
    "    else:\n",
    "        Prec = TP/(TP+FP)\n",
    "    Rec = TP/(TP+FN)\n",
    "    if Prec==0 and Rec == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2*Prec*Rec/(Prec+Rec)\n",
    "    return Acc,Prec,Rec,F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "sample = preprocess(dataset)\n",
    "Y = sample.Exited.values\n",
    "X = sample.drop('Exited',axis=1).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "Y = y_train.reshape(y_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of a neural network\n",
    "nn = NeuralNetwork()\n",
    "# Add Layers (Input layer is created by default)\n",
    "nn.add_layer((9, 7))\n",
    "nn.add_layer((7, 5))\n",
    "nn.add_layer((5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 1) (7200, 9, 1)\n",
      "1000 0.0827918120687047\n",
      "2000 0.0654924890428548\n",
      "3000 0.06651721575952736\n",
      "4000 0.0628677745213538\n",
      "5000 0.0629423944089253\n",
      "6000 0.06819897205951828\n",
      "7000 0.06193877345930545\n",
      "8000 0.06257866936375076\n",
      "9000 0.06358192910926418\n",
      "Error =  0.11169834505953691\n",
      "Epoches needed to train =  10000\n"
     ]
    }
   ],
   "source": [
    "training_data = X\n",
    "training_labels = Y\n",
    "print(training_labels.shape,training_data.shape)\n",
    "error, iteration = nn.train(training_data, training_labels,10000,learning_rate=0.005)\n",
    "print('Error = ', np.mean(np.abs(error[-4:])))\n",
    "print('Epoches needed to train = ', iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 0.8395833333333333\n",
      "Training Precision 0.6310517529215359\n",
      "Training Recall 0.5146358066712049\n",
      "Training F1 0.5669291338582678\n"
     ]
    }
   ],
   "source": [
    "Acc,Prec,Rec,F1 = predict_accuracy(training_data,training_labels)\n",
    "print(\"Training Accuracy\",Acc)\n",
    "print(\"Training Precision\",Prec)\n",
    "print(\"Training Recall\",Rec)\n",
    "print(\"Training F1\",F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "Y = y_test.reshape(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy 0.8261111111111111\n",
      "Testing Precision 0.5830508474576271\n",
      "Testing Recall 0.47513812154696133\n",
      "Testing F1 0.5235920852359208\n"
     ]
    }
   ],
   "source": [
    "testing_data = X\n",
    "testing_labels = Y\n",
    "Acc,Prec,Rec,F1 = predict_accuracy(testing_data,testing_labels)\n",
    "print(\"Testing Accuracy\",Acc)\n",
    "print(\"Testing Precision\",Prec)\n",
    "print(\"Testing Recall\",Rec)\n",
    "print(\"Testing F1\",F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
